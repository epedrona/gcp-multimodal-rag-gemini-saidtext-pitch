{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This section of code is adapted from the \"Building a DIY Multimodal Question Answering System with Vertex AI\" tutorial\n",
    "# Original Author: Lavi Nigam, Google Cloud\n",
    "# Modifications (author Emanuele Pedrona): \n",
    "\n",
    "#Adapted the code to work with a custom PDF (SaidText pitch PDF).\n",
    "#Added specific examples and customizations for prompt questions to Gemini regarding SaidText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDsTUvKjwHBW"
   },
   "source": [
    "# Building a DIY Multimodal Question Answering System with Vertex AI (A Beginner's Guide - Multimodal RAG)\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fqa-ops%2Fbuilding_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YR65ni7TRNYG"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Lavi Nigam](https://github.com/lavinigam-gcp) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QduB55tHOa8N"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ This is a new version of the old mRAG notebook with modifications and new data. You can refer to the old notebook here:  ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70SQT5lKO9dp"
   },
   "source": [
    "[**intro_multimodal_rag.ipynb**](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This guide is your hands-on introduction to creating a question answering system that understands both text and images. We'll build this system from the ground up using Google's Vertex AI, giving you a clear understanding of how it works without relying on complex third-party tools.\n",
    "\n",
    "\n",
    "## Why Build It Yourself?\n",
    "\n",
    "Large Language Models (LLMs) are powerful, but they can seem like a \"black box\". By building our own system, we'll break open that box and explore the core concepts. This will give you the knowledge to customize and optimize every aspect of your question answering system, whether you ultimately choose to code everything yourself or use external libraries.\n",
    "\n",
    "\n",
    "## What We'll Do:\n",
    "\n",
    "* **Focus on Fundamentals**: We'll start with the essential design pattern of \"Retrieval Augmented Generation\" (RAG) – a way to find and use relevant information to answer questions.\n",
    "\n",
    "* **Work with Text and Images**: We'll expand RAG to handle both text and images found in PDF documents. Future guides in this series will explore even more types of data, like videos and audio.\n",
    "\n",
    "* **Use Vertex AI**: We'll only use Google's Vertex AI Embeddings API and Gemini API, ensuring you have complete control and understanding of the building blocks.\n",
    "\n",
    "\n",
    "By the end of this guide, you'll have a solid foundation in building multimodal question answering systems, empowering you to create smarter applications that can understand and respond to a wider range of information.\n",
    "\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision, Gemini 1.0 Pro & Gemini 1.5 Pro models.\n",
    "\n",
    "### Comparing text-based and multimodal RAG\n",
    "\n",
    "Multimodal RAG offers several advantages over text-based RAG:\n",
    "\n",
    "1. **Enhanced knowledge access:** Multimodal RAG can access and process both textual and visual information, providing a richer and more comprehensive knowledge base for the LLM.\n",
    "2. **Improved reasoning capabilities:** By incorporating visual cues, multimodal RAG can make better informed inferences across different types of data modalities.\n",
    "\n",
    "This notebook shows you how to implement DIY RAG with Vertex AI Gemini API\n",
    " and Vertex AI Embeddings API; [text embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings), and [multimodal embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/multimodal-embeddings), to build a document search engine.\n",
    "\n",
    "Through hands-on examples, you will discover how to construct a multimedia-rich metadata repository of your document sources, enabling search, comparison, and reasoning across diverse information streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "This notebook provides a guide to building a document search engine using multimodal retrieval augmented generation (RAG), step by step:\n",
    "\n",
    "1. Extract and store metadata of documents containing both text and images, and generate embeddings the documents\n",
    "2. Search the metadata with text queries to find similar text or images\n",
    "3. Search the metadata with image queries to find similar images\n",
    "4. Using a text query as input, search for contextual answers using both text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnpYxfesh2rI"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXJpXzKrh2rJ"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in ./.local/lib/python3.10/site-packages (1.62.0)\n",
      "Requirement already satisfied: pymupdf in ./.local/lib/python3.10/site-packages (1.24.9)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (13.7.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.32.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.5)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.9 in ./.local/lib/python3.10/site-packages (from pymupdf) (1.24.9)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.65.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.4.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.26.19)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform pymupdf rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 784,
     "status": "ok",
     "timestamp": 1717649530492,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "XRvKdaPDTznN",
    "outputId": "f09a7da9-203a-4992-96e4-fa55f4a1d8bc",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtsU9Bw9h2rL"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GpYEyLsOh2rL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1vKZZoEh2rL"
   },
   "source": [
    "### Define Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1717649555830,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "gJqZ76rJh2rM",
    "outputId": "eef6a25d-f66d-403c-b420-82c541f52ec1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your project ID is: vertexai-test1-414909\n"
     ]
    }
   ],
   "source": [
    "# Define project information\n",
    "\n",
    "import sys\n",
    "\n",
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# if not running on Colab, try to get the PROJECT_ID automatically\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D48gUW5-h2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rtMowvm-yQ97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rich import print as rich_print\n",
    "from rich.markdown import Markdown as rich_Markdown\n",
    "from IPython.display import Markdown, display\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from vertexai.vision_models import MultiModalEmbeddingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-TX_R_xh2rM"
   },
   "source": [
    "### Load the Gemini 1.5 Pro, Gemini 1.5 Pro Flash, Gemini 1.0 Pro Vision and Gemini 1.0 Pro models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HySgZekYzCpW"
   },
   "source": [
    "Learn more about each models and their differences: [here](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts)\n",
    "\n",
    "Learn about the quotas: [here](https://cloud.google.com/vertex-ai/generative-ai/docs/quotas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SvMwSRJJh2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate text model with appropriate name and version\n",
    "text_model = GenerativeModel(\"gemini-1.0-pro\")  # works with text, code\n",
    "\n",
    "# Multimodal models: Choose based on your performance/cost needs\n",
    "multimodal_model_15 = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\"\n",
    ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - complex reasoning\n",
    "\n",
    "# Multimodal models: Choose based on your performance/cost needs\n",
    "multimodal_model_15_flash = GenerativeModel(\n",
    "    \"gemini-1.5-flash-001\"\n",
    ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - faster inference\n",
    "\n",
    "multimodal_model_10 = GenerativeModel(\n",
    "    \"gemini-1.0-pro-vision-001\"\n",
    ")  # works with text, code, video(without audio) and images with 16k input context\n",
    "\n",
    "# Load text embedding model from pre-trained source\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
    "\n",
    "# Load multimodal embedding model from pre-trained source\n",
    "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
    "    \"multimodalembedding\"\n",
    ")  # works with image, image with caption(~32 words), video, video with caption(~32 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7bKCQMFT7JT"
   },
   "source": [
    "#### Get pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KwbL89zcY39N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download documents and images used in this notebook - will take ~30 sec\n",
    "#!gsutil -m -q rsync -r gs://github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_v2 .\n",
    "#print(\"Download completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps1G-cCfpibN"
   },
   "source": [
    "## Building metadata of documents containing text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7uv_PVR1T6B"
   },
   "source": [
    "### The data\n",
    "\n",
    "The source data that you will use in this notebook are:\n",
    "\n",
    "\n",
    "* [Google Cloud TPU Scaling blog](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/Google%20Cloud%20TPU%20blog.pdf)\n",
    "* [Gemini 1.5 Technical Report](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/gemini_v1_5_report_technical.pdf)\n",
    "* [Google Gemma Technical Paper](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/gemma_technical_paper.pdf)\n",
    "* [Med-Gemini Technical Paper](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/med_gemini.pdf)\n",
    "\n",
    "\n",
    "\n",
    "You can also use your data, by first deleting the current files and then placing your files in the `data/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvt0sus5KSNX"
   },
   "source": [
    "### Import helper functions to build metadata\n",
    "\n",
    "Before building the Multimodal Question Answering System with Vertex AI, it's important to have metadata of all the text and images in the document. For references and citations purposes, the metadata should contain essential elements, including page number, file name, image counter, and so on. Hence, as a next step, you will generate embeddings from the metadata, which is required to perform similarity search when querying the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "tStqXX32RNYK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multimodal_qa_with_rag_utils import (\n",
    "    get_document_metadata,\n",
    "    set_global_variable,\n",
    ")\n",
    "\n",
    "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
    "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtaDuBXhFmkL"
   },
   "source": [
    " You can also view the code (`multimodal_qa_with_rag_utils`) [directly](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/utils/multimodal_qa_with_rag_utils.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOAkYN0KlSL"
   },
   "source": [
    "### Extract and store metadata of text and images from a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9hBPPWs5CMd"
   },
   "source": [
    "You just imported a function called `get_document_metadata()`. This function extracts text and image metadata from a document, and returns two dataframes, namely *text_metadata* and *image_metadata*, as outputs. If you want to find out more about how `get_document_metadata()` function is implemented using Gemini and the embedding models, you can take look at the [source code](https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/utils/intro_multimodal_rag_utils.py) directly.\n",
    "\n",
    "The reason for extraction and storing both text metadata and image metadata is that just by using either of the two alone is not sufficient to come out with a relevent answer. For example, the relevant answers could be in visual form within a document, but text-based RAG won't be able to take into consideration of the visual images. You will also be exploring this example later in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnKru0sBh2rN"
   },
   "source": [
    "At the next step, you will use the function to extract and store metadata of text and images from a document. Please note that the following cell may take a few minutes to complete:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8h0XSG_7e5M"
   },
   "source": [
    "**NOTE: Given that we are loading 4 files with roughly 200 pages and approximately 84 images, the cell below will take approximately 7 minutes to run. We recommend loading pre-computed metadata instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350026,
     "status": "ok",
     "timestamp": 1717656088393,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "X8hE0tWD-lf8",
    "outputId": "b094eb22-7c78-41eb-d17c-557631142354",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing pre-existing images folder, since you are running the logic from scratch\n",
      "\n",
      "\n",
      " Processing the file: --------------------------------- data/saidtext_pitch_ita.pdf \n",
      "\n",
      "\n",
      "Processing page: 1\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_0_378.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_1_380.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_2_382.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_3_384.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_4_386.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_5_388.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_6_390.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_7_392.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_8_394.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_9_396.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_10_398.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_11_400.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_12_402.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_13_404.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_14_406.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_15_408.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_16_410.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_17_412.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_18_414.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_19_416.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_20_418.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_21_420.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_22_422.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_23_424.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_24_426.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_25_428.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_26_430.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_27_432.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_28_434.jpeg\n",
      "Extracting image from page: 1, saved as: images/saidtext_pitch_ita.pdf_image_0_29_436.jpeg\n",
      "Processing page: 2\n",
      "Processing page: 3\n",
      "Extracting image from page: 3, saved as: images/saidtext_pitch_ita.pdf_image_2_0_15.jpeg\n",
      "Extracting image from page: 3, saved as: images/saidtext_pitch_ita.pdf_image_2_1_17.jpeg\n",
      "Extracting image from page: 3, saved as: images/saidtext_pitch_ita.pdf_image_2_2_17.jpeg\n",
      "Extracting image from page: 3, saved as: images/saidtext_pitch_ita.pdf_image_2_3_19.jpeg\n",
      "Extracting image from page: 3, saved as: images/saidtext_pitch_ita.pdf_image_2_4_21.jpeg\n",
      "Extracting image from page: 3, saved as: images/saidtext_pitch_ita.pdf_image_2_5_23.jpeg\n",
      "Extracting image from page: 3, saved as: images/saidtext_pitch_ita.pdf_image_2_6_25.jpeg\n",
      "Processing page: 4\n",
      "Extracting image from page: 4, saved as: images/saidtext_pitch_ita.pdf_image_3_0_32.jpeg\n",
      "Extracting image from page: 4, saved as: images/saidtext_pitch_ita.pdf_image_3_1_34.jpeg\n",
      "Extracting image from page: 4, saved as: images/saidtext_pitch_ita.pdf_image_3_2_36.jpeg\n",
      "Extracting image from page: 4, saved as: images/saidtext_pitch_ita.pdf_image_3_3_38.jpeg\n",
      "Processing page: 5\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_0_42.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_1_44.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_2_46.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_3_48.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_4_50.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_5_52.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_6_54.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_7_56.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_8_58.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_9_60.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_10_62.jpeg\n",
      "Extracting image from page: 5, saved as: images/saidtext_pitch_ita.pdf_image_4_11_64.jpeg\n",
      "Processing page: 6\n",
      "Processing page: 7\n",
      "Extracting image from page: 7, saved as: images/saidtext_pitch_ita.pdf_image_6_0_70.jpeg\n",
      "Processing page: 8\n",
      "Extracting image from page: 8, saved as: images/saidtext_pitch_ita.pdf_image_7_0_74.jpeg\n",
      "Processing page: 9\n",
      "Extracting image from page: 9, saved as: images/saidtext_pitch_ita.pdf_image_8_0_78.jpeg\n",
      "Processing page: 10\n",
      "Processing page: 11\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_0_84.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_1_86.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_2_88.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_3_90.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_4_92.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_5_94.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_6_96.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_7_98.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_8_100.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_9_102.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_10_104.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_11_106.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_12_108.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_13_110.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_14_112.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_15_114.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_16_116.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_17_118.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_18_120.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_19_122.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_20_124.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_21_126.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_22_128.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_23_130.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_24_132.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_25_134.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_26_136.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_27_138.jpeg\n",
      "Extracting image from page: 11, saved as: images/saidtext_pitch_ita.pdf_image_10_28_140.jpeg\n",
      "Processing page: 12\n",
      "Extracting image from page: 12, saved as: images/saidtext_pitch_ita.pdf_image_11_0_144.jpeg\n",
      "Processing page: 13\n",
      "Extracting image from page: 13, saved as: images/saidtext_pitch_ita.pdf_image_12_0_148.jpeg\n",
      "Processing page: 14\n",
      "Extracting image from page: 14, saved as: images/saidtext_pitch_ita.pdf_image_13_0_152.jpeg\n",
      "Processing page: 15\n",
      "Processing page: 16\n",
      "Processing page: 17\n",
      "Extracting image from page: 17, saved as: images/saidtext_pitch_ita.pdf_image_16_0_172.jpeg\n",
      "Extracting image from page: 17, saved as: images/saidtext_pitch_ita.pdf_image_16_1_174.jpeg\n",
      "Processing page: 18\n",
      "Extracting image from page: 18, saved as: images/saidtext_pitch_ita.pdf_image_17_0_178.jpeg\n",
      "Extracting image from page: 18, saved as: images/saidtext_pitch_ita.pdf_image_17_1_180.jpeg\n",
      "Extracting image from page: 18, saved as: images/saidtext_pitch_ita.pdf_image_17_2_182.jpeg\n",
      "Extracting image from page: 18, saved as: images/saidtext_pitch_ita.pdf_image_17_3_184.jpeg\n",
      "Processing page: 19\n",
      "Processing page: 20\n",
      "Extracting image from page: 20, saved as: images/saidtext_pitch_ita.pdf_image_19_0_201.jpeg\n",
      "Extracting image from page: 20, saved as: images/saidtext_pitch_ita.pdf_image_19_1_203.jpeg\n",
      "Extracting image from page: 20, saved as: images/saidtext_pitch_ita.pdf_image_19_2_205.jpeg\n",
      "Extracting image from page: 20, saved as: images/saidtext_pitch_ita.pdf_image_19_3_207.jpeg\n",
      "Extracting image from page: 20, saved as: images/saidtext_pitch_ita.pdf_image_19_4_209.jpeg\n",
      "Extracting image from page: 20, saved as: images/saidtext_pitch_ita.pdf_image_19_5_211.jpeg\n",
      "Extracting image from page: 20, saved as: images/saidtext_pitch_ita.pdf_image_19_6_213.jpeg\n",
      "Processing page: 21\n",
      "Extracting image from page: 21, saved as: images/saidtext_pitch_ita.pdf_image_20_0_217.jpeg\n",
      "Extracting image from page: 21, saved as: images/saidtext_pitch_ita.pdf_image_20_1_219.jpeg\n",
      "Extracting image from page: 21, saved as: images/saidtext_pitch_ita.pdf_image_20_2_221.jpeg\n",
      "Extracting image from page: 21, saved as: images/saidtext_pitch_ita.pdf_image_20_3_223.jpeg\n",
      "Processing page: 22\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_0_230.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_1_232.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_2_234.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_3_236.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_4_238.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_5_240.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_6_230.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_7_242.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_8_244.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_9_246.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_10_248.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_11_250.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_12_252.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_13_254.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_14_256.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_15_258.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_16_260.jpeg\n",
      "Extracting image from page: 22, saved as: images/saidtext_pitch_ita.pdf_image_21_17_262.jpeg\n",
      "Processing page: 23\n",
      "Extracting image from page: 23, saved as: images/saidtext_pitch_ita.pdf_image_22_0_273.jpeg\n",
      "Extracting image from page: 23, saved as: images/saidtext_pitch_ita.pdf_image_22_1_275.jpeg\n",
      "Extracting image from page: 23, saved as: images/saidtext_pitch_ita.pdf_image_22_2_277.jpeg\n",
      "Processing page: 24\n",
      "Extracting image from page: 24, saved as: images/saidtext_pitch_ita.pdf_image_23_0_292.jpeg\n",
      "\n",
      " \n",
      " Sleeping for  5  sec before processing the next document to avoid quota issues. You can disable it: \"add_sleep_after_document = False\"  \n",
      "\n",
      "\n",
      " --- Completed processing. ---\n",
      "CPU times: user 4.11 s, sys: 471 ms, total: 4.59 s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Specify the PDF folder with multiple PDF ~7m\n",
    "\n",
    "print(\n",
    "    \"Removing pre-existing images folder, since you are running the logic from scratch\"\n",
    ")\n",
    "! rm -rf images/\n",
    "\n",
    "pdf_folder_path = \"data/\"  # if running in Vertex AI Workbench.\n",
    "\n",
    "# Specify the image description prompt. Change it\n",
    "# image_description_prompt = \"\"\"Explain what is going on in the image.\n",
    "# If it's a table, extract all elements of the table.\n",
    "# If it's a graph, explain the findings in the graph.\n",
    "# Do not include any numbers that are not mentioned in the image.\n",
    "# \"\"\"\n",
    "\n",
    "image_description_prompt = \"\"\"You are a technical image analysis expert. You will be provided with various types of images extracted from documents like research papers, technical blogs, and more.\n",
    "Your task is to generate concise, accurate descriptions of the images without adding any information you are not confident about.\n",
    "Focus on capturing the key details, trends, or relationships depicted in the image.\n",
    "\n",
    "Important Guidelines:\n",
    "* Prioritize accuracy:  If you are uncertain about any detail, state \"Unknown\" or \"Not visible\" instead of guessing.\n",
    "* Avoid hallucinations: Do not add information that is not directly supported by the image.\n",
    "* Be specific: Use precise language to describe shapes, colors, textures, and any interactions depicted.\n",
    "* Consider context: If the image is a screenshot or contains text, incorporate that information into your description.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Extract text and image metadata from the PDF document\n",
    "text_metadata_df, image_metadata_df = get_document_metadata(\n",
    "    multimodal_model_15,  # we are passing Gemini 1.5 Pro\n",
    "    pdf_folder_path,\n",
    "    image_save_dir=\"images\",\n",
    "    image_description_prompt=image_description_prompt,\n",
    "    embedding_size=1408,\n",
    "    # add_sleep_after_page = True, # Uncomment this if you are running into API quota issues\n",
    "    # sleep_time_after_page = 5,\n",
    "    add_sleep_after_document=True,  # Uncomment this if you are running into API quota issues\n",
    "    sleep_time_after_document=5,  # Increase the value in seconds, if you are still getting quota issues. It will slow down the processing.\n",
    "    # generation_config = # see next cell\n",
    "    # safety_settings =  # see next cell\n",
    ")\n",
    "\n",
    "print(\"\\n\\n --- Completed processing. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWtOx1wb86Az"
   },
   "source": [
    "If you would like to pass additional parameters to Gemini while building metadata, here are some options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QlNK0o2DRNYK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Parameters for Gemini API call.\n",
    "# # reference for parameters: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n",
    "\n",
    "# generation_config=  GenerationConfig(temperature=0.2, max_output_tokens=2048)\n",
    "\n",
    "# # Set the safety settings if Gemini is blocking your content or you are facing \"ValueError(\"Content has no parts\")\" error or \"Exception occurred\" in your data.\n",
    "# # ref for settings and thresholds: https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes\n",
    "\n",
    "# safety_settings = {\n",
    "#                   HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#                   }\n",
    "\n",
    "# # You can also pass parameters and safety_setting to \"get_gemini_response\" function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tW3Ci1IL8wSW"
   },
   "source": [
    "### Load pre-computed metadata of text and images from source document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c10dCy6Cig3H"
   },
   "source": [
    "**If you are facing constant issues with Quota or want to focus on the outputs, you should load pre-computed metadata.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "w1AGYOYb0In7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Load the pickle file\n",
    "# with open(\"mrag_metadata.pkl\", \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# # Extract the DataFrames\n",
    "# text_metadata_df = data[\"text_metadata\"]\n",
    "# image_metadata_df = data[\"image_metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miBBoEXwh2rN"
   },
   "source": [
    "#### Inspect the processed text metadata\n",
    "\n",
    "\n",
    "The following cell will produce a metadata table which describes the different parts of text metadata, including:\n",
    "\n",
    "- **text**: the original text from the page\n",
    "- **text_embedding_page**: the embedding of the original text from the page\n",
    "- **chunk_text**: the original text divided into smaller chunks\n",
    "- **chunk_number**: the index of each text chunk\n",
    "- **text_embedding_chunk**: the embedding of each text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1717656148133,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "6t3AIGFar8Mo",
    "outputId": "f85c6d80-93e1-4cdd-ca77-efd690b67bf3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>page_num</th>\n",
       "      <th>text</th>\n",
       "      <th>text_embedding_page</th>\n",
       "      <th>chunk_number</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>text_embedding_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Voice-Driven AI\\nReshaping the Future of Manuf...</td>\n",
       "      <td>[-0.0029605894815176725, -0.01030410174280405,...</td>\n",
       "      <td>1</td>\n",
       "      <td>Voice-Driven AI\\nReshaping the Future of Manuf...</td>\n",
       "      <td>[-0.0029605894815176725, -0.01030410174280405,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>Intro\\nSaaS che riduce\\ni tempi di \\nproduzion...</td>\n",
       "      <td>[0.006873331964015961, 0.004846159368753433, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>Intro\\nSaaS che riduce\\ni tempi di \\nproduzion...</td>\n",
       "      <td>[0.006873331964015961, 0.004846159368753433, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>Troppi messaggi\\nI lavoratori spendono fino al...</td>\n",
       "      <td>[0.006359332241117954, 0.024240709841251373, -...</td>\n",
       "      <td>1</td>\n",
       "      <td>Troppi messaggi\\nI lavoratori spendono fino al...</td>\n",
       "      <td>[0.006359332241117954, 0.024240709841251373, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>SAID TEXT\\nSaidText sfrutta il riconoscimento ...</td>\n",
       "      <td>[-9.709423466119915e-05, 0.017527582123875618,...</td>\n",
       "      <td>1</td>\n",
       "      <td>SAID TEXT\\nSaidText sfrutta il riconoscimento ...</td>\n",
       "      <td>[-9.709423466119915e-05, 0.017527582123875618,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>5</td>\n",
       "      <td>MANUFACTURING \\nProduttori interessati in solu...</td>\n",
       "      <td>[-0.026080278679728508, 0.016656098887324333, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>MANUFACTURING \\nProduttori interessati in solu...</td>\n",
       "      <td>[-0.026080278679728508, 0.016656098887324333, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                file_name  page_num  \\\n",
       "0  saidtext_pitch_ita.pdf         1   \n",
       "1  saidtext_pitch_ita.pdf         2   \n",
       "2  saidtext_pitch_ita.pdf         3   \n",
       "3  saidtext_pitch_ita.pdf         4   \n",
       "4  saidtext_pitch_ita.pdf         5   \n",
       "\n",
       "                                                text  \\\n",
       "0  Voice-Driven AI\\nReshaping the Future of Manuf...   \n",
       "1  Intro\\nSaaS che riduce\\ni tempi di \\nproduzion...   \n",
       "2  Troppi messaggi\\nI lavoratori spendono fino al...   \n",
       "3  SAID TEXT\\nSaidText sfrutta il riconoscimento ...   \n",
       "4  MANUFACTURING \\nProduttori interessati in solu...   \n",
       "\n",
       "                                 text_embedding_page  chunk_number  \\\n",
       "0  [-0.0029605894815176725, -0.01030410174280405,...             1   \n",
       "1  [0.006873331964015961, 0.004846159368753433, -...             1   \n",
       "2  [0.006359332241117954, 0.024240709841251373, -...             1   \n",
       "3  [-9.709423466119915e-05, 0.017527582123875618,...             1   \n",
       "4  [-0.026080278679728508, 0.016656098887324333, ...             1   \n",
       "\n",
       "                                          chunk_text  \\\n",
       "0  Voice-Driven AI\\nReshaping the Future of Manuf...   \n",
       "1  Intro\\nSaaS che riduce\\ni tempi di \\nproduzion...   \n",
       "2  Troppi messaggi\\nI lavoratori spendono fino al...   \n",
       "3  SAID TEXT\\nSaidText sfrutta il riconoscimento ...   \n",
       "4  MANUFACTURING \\nProduttori interessati in solu...   \n",
       "\n",
       "                                text_embedding_chunk  \n",
       "0  [-0.0029605894815176725, -0.01030410174280405,...  \n",
       "1  [0.006873331964015961, 0.004846159368753433, -...  \n",
       "2  [0.006359332241117954, 0.024240709841251373, -...  \n",
       "3  [-9.709423466119915e-05, 0.017527582123875618,...  \n",
       "4  [-0.026080278679728508, 0.016656098887324333, ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjIQYI3mh2rO"
   },
   "source": [
    "#### Inspect the processed image metadata\n",
    "\n",
    "The following cell will produce a metadata table which describes the different parts of image metadata, including:\n",
    "* **img_desc**: Gemini-generated textual description of the image.\n",
    "* **mm_embedding_from_text_desc_and_img**: Combined embedding of image and its description, capturing both visual and textual information.\n",
    "* **mm_embedding_from_img_only**: Image embedding without description, for comparison with description-based analysis.\n",
    "* **text_embedding_from_image_description**: Separate text embedding of the generated description, enabling textual analysis and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1717656148875,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "tkHtAYIK-y-q",
    "outputId": "fb17751f-c7c7-4514-8057-87329372e46c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>page_num</th>\n",
       "      <th>img_num</th>\n",
       "      <th>img_path</th>\n",
       "      <th>img_desc</th>\n",
       "      <th>mm_embedding_from_img_only</th>\n",
       "      <th>text_embedding_from_image_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>images/saidtext_pitch_ita.pdf_image_0_0_378.jpeg</td>\n",
       "      <td>The image is entirely black. A faint white lin...</td>\n",
       "      <td>[0.01719152, 0.00548105733, 0.00085720385, -0....</td>\n",
       "      <td>[0.0173872672021389, 0.005338246468454599, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>images/saidtext_pitch_ita.pdf_image_0_1_380.jpeg</td>\n",
       "      <td>The image shows a white background with the te...</td>\n",
       "      <td>[-0.00427523255, 0.0293428563, -0.00296164211,...</td>\n",
       "      <td>[-0.0070267124101519585, -0.004656734876334667...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>images/saidtext_pitch_ita.pdf_image_0_2_382.jpeg</td>\n",
       "      <td>The image is a dark gray circle on a black bac...</td>\n",
       "      <td>[0.0244258884, 0.0309831221, 0.0131219393, 0.0...</td>\n",
       "      <td>[0.025137899443507195, 0.030512524768710136, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>images/saidtext_pitch_ita.pdf_image_0_3_384.jpeg</td>\n",
       "      <td>The image shows a black circle surrounded by a...</td>\n",
       "      <td>[0.00397431571, 0.00512644975, 0.000962337712,...</td>\n",
       "      <td>[0.0027490551583468914, 0.020363206043839455, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>saidtext_pitch_ita.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>images/saidtext_pitch_ita.pdf_image_0_4_386.jpeg</td>\n",
       "      <td>The image is too blurry to interpret. It shows...</td>\n",
       "      <td>[0.0164599828, 0.0135841873, 0.014787266, 0.00...</td>\n",
       "      <td>[0.03547249734401703, 0.001516010263003409, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                file_name  page_num  img_num  \\\n",
       "0  saidtext_pitch_ita.pdf         1        1   \n",
       "1  saidtext_pitch_ita.pdf         1        2   \n",
       "2  saidtext_pitch_ita.pdf         1        3   \n",
       "3  saidtext_pitch_ita.pdf         1        4   \n",
       "4  saidtext_pitch_ita.pdf         1        5   \n",
       "\n",
       "                                           img_path  \\\n",
       "0  images/saidtext_pitch_ita.pdf_image_0_0_378.jpeg   \n",
       "1  images/saidtext_pitch_ita.pdf_image_0_1_380.jpeg   \n",
       "2  images/saidtext_pitch_ita.pdf_image_0_2_382.jpeg   \n",
       "3  images/saidtext_pitch_ita.pdf_image_0_3_384.jpeg   \n",
       "4  images/saidtext_pitch_ita.pdf_image_0_4_386.jpeg   \n",
       "\n",
       "                                            img_desc  \\\n",
       "0  The image is entirely black. A faint white lin...   \n",
       "1  The image shows a white background with the te...   \n",
       "2  The image is a dark gray circle on a black bac...   \n",
       "3  The image shows a black circle surrounded by a...   \n",
       "4  The image is too blurry to interpret. It shows...   \n",
       "\n",
       "                          mm_embedding_from_img_only  \\\n",
       "0  [0.01719152, 0.00548105733, 0.00085720385, -0....   \n",
       "1  [-0.00427523255, 0.0293428563, -0.00296164211,...   \n",
       "2  [0.0244258884, 0.0309831221, 0.0131219393, 0.0...   \n",
       "3  [0.00397431571, 0.00512644975, 0.000962337712,...   \n",
       "4  [0.0164599828, 0.0135841873, 0.014787266, 0.00...   \n",
       "\n",
       "               text_embedding_from_image_description  \n",
       "0  [0.0173872672021389, 0.005338246468454599, -0....  \n",
       "1  [-0.0070267124101519585, -0.004656734876334667...  \n",
       "2  [0.025137899443507195, 0.030512524768710136, -...  \n",
       "3  [0.0027490551583468914, 0.020363206043839455, ...  \n",
       "4  [0.03547249734401703, 0.001516010263003409, -0...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBhoOkutUtPr"
   },
   "source": [
    "### Import the helper functions to implement RAG\n",
    "\n",
    "You will be importing the following functions which will be used in the remainder of this notebook to implement RAG:\n",
    "\n",
    "* **get_similar_text_from_query():** Given a text query, finds text from the document which are relevant, using cosine similarity algorithm. It uses text embeddings from the metadata to compute and the results can be filtered by top score, page/chunk number, or embedding size.\n",
    "* **print_text_to_text_citation():** Prints the source (citation) and details of the retrieved text from the `get_similar_text_from_query()` function.\n",
    "* **get_similar_image_from_query():** Given an image path or an image, finds images from the document which are relevant. It uses image embeddings from the metadata.\n",
    "* **print_text_to_image_citation():** Prints the source (citation) and the details of retrieved images from the `get_similar_image_from_query()` function.\n",
    "* **get_gemini_response():** Interacts with a Gemini model to answer questions based on a combination of text and image inputs.\n",
    "* **display_images():**  Displays a series of images provided as paths or PIL Image objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Tngn_vrIKdE1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from multimodal_qa_with_rag_utils import (\n",
    "    get_similar_text_from_query,\n",
    "    print_text_to_text_citation,\n",
    "    get_similar_image_from_query,\n",
    "    print_text_to_image_citation,\n",
    "    get_gemini_response,\n",
    "    display_images,\n",
    "    get_answer_from_qa_system,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9jGEj6DY1Rj"
   },
   "source": [
    "Before implementing a Multimodal Question Answering System with Vertex AI, let's explore what you can achieve with just text or image embeddings. This will set the foundation for implementing a multimodal Retrieval Augmented Generation (RAG) system, which you will do later in this notebook.\n",
    "\n",
    "You can also use these essential elements together to build applications for multimodal use cases, extracting meaningful information from documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHuLlEvSKFWt"
   },
   "source": [
    "## Text Search\n",
    "\n",
    "Let's start the search with a simple question and see if the simple text search using text embeddings can answer it. The expected answer is to show the value of basic and diluted net income per share of Google for different share types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5mrFVhtCut7t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"In quale ambito si usa SaidText?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWw7-AIar-S8"
   },
   "source": [
    "### Search similar text with text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1717656148876,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "eEzP6Yyv7N-G",
    "outputId": "4cb21b58-fcec-4109-9258-43faa1017cb6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mCitation 1: Matched text: \n",
      "\u001b[0m\n",
      "\u001b[94mscore: \u001b[0m 0.76\n",
      "\u001b[94mfile_name: \u001b[0m saidtext_pitch_ita.pdf\n",
      "\u001b[94mpage_number: \u001b[0m 11\n",
      "\u001b[94mchunk_number: \u001b[0m 1\n",
      "\u001b[94mchunk_text: \u001b[0m SaidText cattura e \n",
      "converte con precisione\n",
      "le comunicazioni vocali\n",
      "in testo. \n",
      "Una volta elaborate, le \n",
      "trascrizioni vengono\n",
      "categorizzate in modo \n",
      "ordinato e rese \n",
      "accessibili attraverso\n",
      "una dashboard \n",
      "amministrativa intuitiva.\n",
      "MOBILE APP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matching user text query with \"chunk_embedding\" to find relevant chunks.\n",
    "matching_results_text = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=3,\n",
    "    chunk_text=True,\n",
    ")\n",
    "\n",
    "# Print the matched text citations\n",
    "print_text_to_text_citation(\n",
    "    matching_results_text, print_top=True, chunk_text=True\n",
    ")  # print_top=False to see all text matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KY1J2sHr-N8f"
   },
   "source": [
    "### Get answer with text-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ORCistIdDWoE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All relevant text chunk found across documents based on user query\n",
    "context = \"\\n\".join(\n",
    "    [value[\"chunk_text\"] for key, value in matching_results_text.items()]\n",
    ")\n",
    "\n",
    "prompt = f\"\"\"Answer the question with the given context. If the specific answer is not in the context, please answer \"I don't know\".\n",
    "Question: {query}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "GKD-Ew8IM287",
    "tags": []
   },
   "outputs": [],
   "source": [
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 1478,
     "status": "ok",
     "timestamp": 1717656150350,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "m8jyc5_SAwlF",
    "outputId": "86047b5b-1176-4480-d61e-9e388259bc5e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **** Result: ***** \n",
      "\n",
      "CPU times: user 17.1 ms, sys: 1.02 ms, total: 18.1 ms\n",
      "Wall time: 1.18 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">SaidText si usa negli <span style=\"font-weight: bold\">ambienti industriali</span> e per il <span style=\"font-weight: bold\">customer care</span>.                                                 \n",
       "</pre>\n"
      ],
      "text/plain": [
       "SaidText si usa negli \u001b[1mambienti industriali\u001b[0m e per il \u001b[1mcustomer care\u001b[0m.                                                 \n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Generate response with Gemini 1.5 Pro\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "rich_Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15,\n",
    "        model_input=prompt,\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    },
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1717656151570,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "0rKLLKt1An97",
    "outputId": "105fdca8-b21f-43e8-a69e-db8e740dc942",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.6 ms, sys: 8.75 ms, total: 27.3 ms\n",
      "Wall time: 301 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Ambienti industriali.                                                                                              \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Ambienti industriali.                                                                                              \n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Generate response with Gemini 1.5 FLash\n",
    "rich_Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15_flash,\n",
    "        model_input=prompt,\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=0.1),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXm271jdD-Rl"
   },
   "source": [
    "### Search similar images with text query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPxwfyVrr9-G"
   },
   "source": [
    "Since plain text search and RAG didn't provide the detailed answer and the information may be visually represented in a table or another image format, you can use multimodal capability of Gemini 1.0 Pro Vision or Gemini 1.5 Pro model for the similar task.\n",
    "\n",
    "The goal here also is to find an image similar to the text query. You may also print the citations to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "nwXpqMSq4ppr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"In quale ambito si usa SaidText?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1717656152420,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "knj4qQ4xni24",
    "outputId": "ce22e3ec-aca8-4915-cfc9-20d1bcda8485",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **** Result: ***** \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAdAEkDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDxPStBvtYY/ZkAjU4aRzhQfT3rRvPBWqWsJljMVwFGSsZO78ARzXovhXS4ZLWxs0dY4/KDE/3jgE49zmtnVNHSxjeVLlCPM2iMn5gMA4+vP5Yr36eW4dRUJt8zR408fW5nKCXKj5+IIOD1pK3fF9vHb+IpxEAA6q5A7Ejn/GsXaNueTxXiVqbp1HB9GetTnzwU11GUU8JyOcigJwcntnFZljKKcoUg5HQZ60bDjPQdaAG0U8Jzgn8qZgf3v0oA7fwz4ugtrWKzv3aMxcRTAZGOwPpiumvdesobA6jJciWNyShB5kb0FeRU9pXdVV3ZggwoJzge1epRzWrThyNXstGefVy6nOfMnbuTahey6jfzXc335Gzgdh2H5VBvOP0zTaK8yUnJtvc70klZDvMPtRvOO3pTaKQxQcZ96XccY4ptFADt5znik3f7K/lSUUAf/9k=\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=73x29>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAnAGkDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/q3Zabe6i+y0tpJiOu1eB9T0FP0jTm1XVYLNTt8xvmb0A5J/KvZdM02OBIbCwhCrkKqjuemSf6mu/B4L6xeUnaKOPFYr2NopXbPIL3w5q2nx+ZcWUixjkuuGA+pHSsuvdyrDOVI7HIrzbxvoUWn3Ed9aoEhnJV0AwFf2+v8AStsZl3sYe0pu6MsLjvay5JqzORopQCTgUrIV6ivKPRG0UAZOKUghsd6AEop207tvel8pvT9aAGUUUUAFFKQQeRSUAdF4JuEt/E0Ic481GjB9yOP5V7b4du4Le78uVQHlZQsh7c9Pof6V85I7RyK6MVZSCCOoNeh6N47tZYFi1TdFMowZQuVf3IHQ162AxFP2bo1Ha55uMoTc1Vgrnp9/q1jPazRpbh3ZhhiMZP8Ae+uPlrzL4hTomkW0BP7ySbcB7AHJ/UV0MmsWEel/2ibhTa4OHH8XbAHc5ryrXdYl1vUnuXBVB8sSf3V/xrqx1WnRoOnF3cv6uc+EpzqVeeS0Rnx538U7GRwCOehqLpSliepNfPntErAY6dD6UEYyQATn0qLc3qaNzepoAdL/AKw0rfcSoySTk0ZOMUATMAARjp04obhdwUZ71FubGMmjceeTzQA+XO7pxUdKST1NJQAUUUUATS3dxPFFFLM7xxLtjUnhR7CoaKKbbe4JW2CiiikAUUUUAFFFFABRRRQAUUUUAf/Z\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=105x39>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAtAHEDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDwCporW4uBmGCWQf7CE/yrpvCnhyO/X7depugDYjjPRyOpPtXeRxrEgSNAiLwFUYAr1cJlcq0OebsmedicwjSlyRV2eNSRSRNtkRkb0YEGmV7FeWNtqEJiuoVlQ+o5H0PavMtf0ZtG1Aw5LQuN0TnuPQ+4rPGZdPDLmTujTC42Nd8trMyqKKK847QooooAKKKKACiiigAooooAKKKKAPWtDRY9BsQnTyFP5jJr0ODRrNNJkh3CQOu9pAepVJSD17f0ryPwbrMdxYrp0rATw58sH+NPb6V2kWo3MNlJaJIwjcg9TlR82QPY7jmvrIXxGHg6Ttt/XyPnJ/ua0lURoaxpNtYwRSQ3CE42lf8Anpgn5h16/lxXm/j1FOnWkhA3iYgH2I5/kK7B5Hk2hmLbVCrk9B6CvNfGGrLqGpLbwtuhtsrkdGbv/hWWPl7LCuM3dsvBR9piFKCskc4MZGelSnPOACvpUQODmnhlB3AHPpXzB9AIqg9z+ApdpCuM9KA42gHPHpQXB3deaAFVQrDnmmZw5I9aeHXIYg5qM8kmgCVnIRT3NJszjJ5PtTSwKAelLvBAznj0oAAmcjPIPNKqr83fFIr7Rx1JoDKC3BwaAG4X1NFHye9FACo7RuHRirKchgcEGuisPGGqxvFDLNFIhYAvMvIGe5Fc3RWtKvUpO8HYzqUoVFaaudn4h8XrLG9ppjMFbh5+hI9F/wAa4yiiqxGIqV5802KjRhRjywCiiisDUKKKKACiiigAooooAKKKKACiiigD/9k=\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=113x45>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAIAB8DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDyeHVLKOPaUt5OQQX3AgenBHFWYNX0jnz4YRzxsDnjH+8KKKu4rEy6voG5iYgATwu0nA/77ps9xp95aD7DbSM27BYREg+33j7dqKKLhY//2Q==\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB8AAAAICAIAAACOH7NNAAABzUlEQVR4AY1S24rbMBDVxbf4tnEDCc72Je1C2ZTQC6XPS389b+0+5gdClm7BSyDEsSVZlnpkLy1tXzoYeXTmaM5oRrSYvVyUpbW26zpKKeccPgz+uP5yjIHrzBjDXFTDf+YQTojDgRB4DMft09MPerv53Pc90OVyKYQ4n88Ie54HsSAIpJRYR4LTtAgSkHutOWe+74dhCKaUCng8mbRtizAhVimJvzcK4lxZlvP5HHl3u91ms8EZ2H6/X6/XcLTWlvRIl8SJ7vX9t/sP7z9WVZVlGWOQ8R4fv0MASeq6DsPJdru9XC70zdtPrp7BwENnlFJB4KNESKLqNE2LojidTlkeo5TFYvHwcFBSh36UX+VN0wDMsgTcthV5nmNb15eqOkrReZz7KA2XhWlt8FHKlOpdF5DfOGe1Wh0Oh9c3rzql0L04zq6vixfTmbEmiiIIp2mCEDIASZKkacTx+JXxjq5u3gVRSImbxt9mAT7fAN2njCr0lxKGqRGLHuKWGO+QFGUYXB2lwBgnnFMhGur5ydV0OoxrGNkfCk7SoeM0XchNDOl+g8MjcRHHcZtoEhFroNG0NbZ4TP/mdXxCzPj7/zUKw7svd0ZTIdvZrPgJ32gWDzhMPoEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=31x8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matching_results_image = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding\n",
    "    image_emb=False,  # Use text embedding instead of image embedding\n",
    "    top_n=5,\n",
    "    embedding_size=1408,\n",
    ")\n",
    "\n",
    "# Markdown(print_text_to_image_citation(matching_results_image, print_top=True))\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# Display the top matching image\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image[0][\"img_path\"],\n",
    "        matching_results_image[1][\"img_path\"],\n",
    "        matching_results_image[2][\"img_path\"],\n",
    "        matching_results_image[3][\"img_path\"],\n",
    "    ],\n",
    "    resize_ratio=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 14967,
     "status": "ok",
     "timestamp": 1717656167385,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "HuRD1lZ8RNYP",
    "outputId": "9e3ea7be-b760-4771-b028-6da6b661c11f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **** Result: ***** \n",
      "\n",
      "CPU times: user 22.6 ms, sys: 0 ns, total: 22.6 ms\n",
      "Wall time: 2.77 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on the logo with the name \"SaidText\" and a speech bubble containing a soundwave, SaidText is likely used in  \n",
       "the field of <span style=\"font-weight: bold\">voice-to-text transcription or dictation</span>.                                                             \n",
       "\n",
       "Here's why:                                                                                                        \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Speech Bubble:</span> The speech bubble icon is a common symbol for communication, especially text messaging or chat.  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Soundwave:</span> The soundwave inside the speech bubble signifies audio input.                                        \n",
       "\n",
       "Therefore, SaidText likely converts spoken words into written text.                                                \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Based on the logo with the name \"SaidText\" and a speech bubble containing a soundwave, SaidText is likely used in  \n",
       "the field of \u001b[1mvoice-to-text transcription or dictation\u001b[0m.                                                             \n",
       "\n",
       "Here's why:                                                                                                        \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSpeech Bubble:\u001b[0m The speech bubble icon is a common symbol for communication, especially text messaging or chat.  \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSoundwave:\u001b[0m The soundwave inside the speech bubble signifies audio input.                                        \n",
       "\n",
       "Therefore, SaidText likely converts spoken words into written text.                                                \n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "instruction = f\"\"\"Answer the question and explain results with the given Image:\n",
    "Question: {query}\n",
    "Image:\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the model input\n",
    "model_input = [\n",
    "    instruction,\n",
    "    # passing all matched images to Gemini\n",
    "    \"Image:\",\n",
    "    matching_results_image[0][\"image_object\"],\n",
    "    \"Description:\",\n",
    "    matching_results_image[0][\"image_description\"],\n",
    "    \"Image:\",\n",
    "    matching_results_image[1][\"image_object\"],\n",
    "    \"Description:\",\n",
    "    matching_results_image[1][\"image_description\"],\n",
    "    \"Image:\",\n",
    "    matching_results_image[2][\"image_object\"],\n",
    "    \"Description:\",\n",
    "    matching_results_image[2][\"image_description\"],\n",
    "    \"Image:\",\n",
    "    matching_results_image[3][\"image_object\"],\n",
    "    \"Description:\",\n",
    "    matching_results_image[3][\"image_description\"],\n",
    "]\n",
    "\n",
    "# Generate Gemini response with streaming output\n",
    "rich_Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15,\n",
    "        model_input=model_input,\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1717656167385,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "uykUaQvIRNYP",
    "outputId": "b95a7131-d8b8-4aef-8b90-b7387f814417",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mCitation 1: Matched image path, page number and page text: \n",
      "\u001b[0m\n",
      "\u001b[94mscore: \u001b[0m 0.66\n",
      "\u001b[94mfile_name: \u001b[0m saidtext_pitch_ita.pdf\n",
      "\u001b[94mpath: \u001b[0m images/saidtext_pitch_ita.pdf_image_20_1_219.jpeg\n",
      "\u001b[94mpage number: \u001b[0m 21\n",
      "\u001b[94mpage text: \u001b[0m Equity Story\n",
      "Founders Equity\n",
      "Investimento iniziale dei fondatori alla costituzione: 100.000\n",
      "Pre-Seed Round\n",
      "SaidText ha ottenuto un finanziamento pre-seed di \n",
      "188.000 euro da CDP Cassa Depositi e Prestiti, \n",
      "attraverso la selezione per l'esclusivo Programma di \n",
      "Accelerazione Forward Factory CDP, che ha avuto un \n",
      "tasso di accettazione competitivo dello 0,2% per le \n",
      "startup di manifattura digitale nel 2023.\n",
      "\n",
      "\u001b[94mimage description: \u001b[0m The image shows the words \"SaidText\" next to a blue chat bubble icon. The icon contains a white outline of a soundwave. \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;35mNone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## you can check the citations to probe further.\n",
    "## check the \"image description:\" which is a description extracted through Gemini which helped search our query.\n",
    "rich_print(print_text_to_image_citation(matching_results_image, print_top=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDd9rE4NrRod"
   },
   "source": [
    "## Image Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJL6ElyEy4mc"
   },
   "source": [
    "### Search similar image with image input [using multimodal image embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReKjHleFxUu9"
   },
   "source": [
    "Imagine searching for images, but instead of typing words, you use an actual image as the clue.\n",
    "\n",
    "Think of it like searching with a mini-map instead of a written address.\n",
    "It's a different way to ask, \"Show me more stuff like this\".\n",
    "\n",
    "So, instead of typing \"various example of gemini 1.5 long context\", you show a picture of that image and say, \"Find me more like this\"\n",
    "\n",
    "For demonstration purposes, we will only be finding similar images that show the various features of Gemini in a single document below. However, you can scale this design pattern to match (find relevant images) across multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 926
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1717656167385,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "DJhhS5eZw7QI",
    "outputId": "b8aa4e36-289d-4637-b884-430e33d52b45",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can find a similar image as per the images you have in the metadata.\n",
    "\n",
    "image_query_path = \"images/gemini_v1_5_report_technical.pdf_image_5_0_148.jpeg\"\n",
    "\n",
    "# Print a message indicating the input image\n",
    "print(\"***Input image from user:***\")\n",
    "\n",
    "# Display the input image\n",
    "Image.load_from_file(image_query_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zBTtGChTmrd"
   },
   "source": [
    "You expect to find images that are similar in terms of \"long context prompts for gemini 1.5 pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "executionInfo": {
     "elapsed": 2294,
     "status": "ok",
     "timestamp": 1717656169675,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "nZcU7vZC-8vr",
    "outputId": "1cd88a31-99a6-4e84-e3cb-1553e20be884",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search for Similar Images Based on Input Image and Image Embedding\n",
    "\n",
    "matching_results_image = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,  # Use query text for additional filtering (optional)\n",
    "    column_name=\"mm_embedding_from_img_only\",  # Use image embedding for similarity calculation\n",
    "    image_emb=True,\n",
    "    image_query_path=image_query_path,  # Use input image for similarity calculation\n",
    "    top_n=3,  # Retrieve top 3 matching images\n",
    "    embedding_size=1408,  # Use embedding size of 1408\n",
    ")\n",
    "\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# Display the Top Matching Image\n",
    "display(\n",
    "    matching_results_image[0][\"image_object\"]\n",
    ")  # Display the top matching image object (Pillow Image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhT17rke15XY"
   },
   "source": [
    "\n",
    "You can also print the citation to see what it has matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1717656169675,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "mksXQoezweg0",
    "outputId": "58398873-5f61-4331-b6db-a043d0bf9aa0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display citation details for the top matching image\n",
    "print_text_to_image_citation(\n",
    "    matching_results_image, print_top=True\n",
    ")  # Print citation details for the top matching image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1717656169676,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "VJWnhDJwI-uO",
    "outputId": "346c6107-1e98-47cb-da6f-ff63eacaf09c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check Other Matched Images (Optional)\n",
    "# You can access the other two matched images using:\n",
    "\n",
    "print(\"---------------Matched Images------------------\\n\")\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image[0][\"img_path\"],\n",
    "        matching_results_image[1][\"img_path\"],\n",
    "        matching_results_image[2][\"img_path\"],\n",
    "    ],\n",
    "    resize_ratio=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvwZIgD84CNc"
   },
   "source": [
    "The ability to identify similar text and images based on user input, using Gemini and embeddings, forms a crucial foundation for development of Multimodal Question Answering System with multimodal RAG design pattern, which you will explore in the coming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUnsv5Co6pJF"
   },
   "source": [
    "### Comparative reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AFbqHiz5vvo"
   },
   "source": [
    "Next, let's apply what you have done so far in doing comparative reasoning.\n",
    "\n",
    "For this example:\n",
    "\n",
    "* **Step 1:** You will search all the images for a specific query\n",
    "\n",
    "* **Step 2:** Send those images to Gemini 1.5 Pro to ask multiple questions, where it has to compare among those images and provide you with answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6AHCSwojyX0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "matching_results_image_query_1 = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=\"Show me all the images that can describe LLMs and TPU v5e scaling\",\n",
    "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding # mm_embedding_from_img_only text_embedding_from_image_description\n",
    "    image_emb=False,  # Use text embedding instead of image embedding\n",
    "    top_n=5,\n",
    "    embedding_size=1408,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1717656169676,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "jja-9iGFRNYQ",
    "outputId": "d9e7a39f-b9f6-4080-a256-fb7315015eae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check Matched Images\n",
    "# You can access the other two matched images using:\n",
    "\n",
    "print(\"---------------Matched Images------------------\\n\")\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image_query_1[0][\"img_path\"],\n",
    "        matching_results_image_query_1[1][\"img_path\"],\n",
    "        matching_results_image_query_1[2][\"img_path\"],\n",
    "        matching_results_image_query_1[3][\"img_path\"],\n",
    "        matching_results_image_query_1[4][\"img_path\"],\n",
    "    ],\n",
    "    resize_ratio=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSR_JWkSC_7p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Task: Answer the following questions in detail, providing clear reasoning and evidence from the images in bullet points.\n",
    "Instructions:\n",
    "1. Analyze the provided images focusing on the relationship between TPU v5e scaling efficiency, LLM model size growth, performance metrics, and quantization effects.\n",
    "2. Answer the following questions in detail, providing clear reasoning and evidence from the images in bullet points\n",
    "3. Cite the image sources to support your explanations. Mention the file name.\n",
    "\n",
    "Additional Considerations:\n",
    "* Clearly define any technical terms (e.g., EMFU, TFLOP/chip/s) within your answers for better understanding.\n",
    "* Use specific examples and data points from the images to support your explanations.\n",
    "* Feel free to request additional information or clarification if the images are unclear or ambiguous.\n",
    "\n",
    "Question:\n",
    " - How does the scaling efficiency of TPU v5e compare to the overall growth in LLM model size over time?\n",
    " - How does the model size impact the observed Per-chip performance and EMFU for a fixed number of TPU v5e chips (e.g., 256)?\n",
    " - For the INT8 Quant training with 32B parameters, how does its high EMFU relate to the observed TFLOP/chip/s?\n",
    " - how does the \"per device batch (seq)\" for a 16B model compare to a 128B model, and how does this affect the \"Total observed Perf\"?\n",
    " - how might the MFU be impacted by increasing LLM model size?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 20161,
     "status": "ok",
     "timestamp": 1717656189834,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "wYkzpB4PTSfm",
    "outputId": "89e33976-ee7f-4f92-9ca7-93cc7204cbed",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generate response with Gemini 1.5 Pro\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "rich_Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15,\n",
    "        model_input=[\n",
    "            prompt,\n",
    "            \"Images:\",\n",
    "            matching_results_image_query_1[0][\"image_object\"],\n",
    "            matching_results_image_query_1[1][\"image_object\"],\n",
    "            matching_results_image_query_1[2][\"image_object\"],\n",
    "            matching_results_image_query_1[3][\"image_object\"],\n",
    "            matching_results_image_query_1[4][\"image_object\"],\n",
    "        ],\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efJPPrzRhvIT"
   },
   "source": [
    "## Building Multimodal QA System with retrieval augmented generation (mRAG)\n",
    "\n",
    "Let's bring everything together to implement multimodal RAG. You will use all the elements that you've explored in previous sections to implement the multimodal RAG. These are the steps:\n",
    "\n",
    "* **Step 1:** The user gives a query in text format where the expected information is available in the document and is embedded in images and text.\n",
    "* **Step 2:** Find all text chunks from the pages in the documents using a method similar to the one you explored in `Text Search`.\n",
    "* **Step 3:** Find all similar images from the pages based on the user query matched with `image_description` using a method identical to the one you explored in `Image Search`.\n",
    "* **Step 4:** Combine all similar text and images found in steps 2 and 3 as `context_text` and `context_images`.\n",
    "* **Step 5:** With the help of Gemini, we can pass the user query with text and image context found in steps 2 & 3. You can also add a specific instruction the model should remember while answering the user query.\n",
    "* **Step 6:** Gemini produces the answer, and you can print the citations to check all relevant text and images used to address the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI62Hzuw_0_b"
   },
   "source": [
    "### Step 1: User query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "XvTKFwOPHLQ_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this time we are not passing any images, but just a simple text query.\n",
    "\n",
    "query = \"\"\"Perche SaidText deve essere usato nel mondo manifatturiero?\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUqlkKUaYvZA"
   },
   "source": [
    "### Step 2: Get all relevant text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "r65yBb5gR_NG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant chunks of text based on the query\n",
    "matching_results_chunks_data = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=20,\n",
    "    chunk_text=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIgXgVIpYzxj"
   },
   "source": [
    "### Step 3: Get all relevant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "wzu5Gf4yR_J4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all relevant images based on user query\n",
    "matching_results_image_fromdescription_data = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",\n",
    "    image_emb=False,\n",
    "    top_n=10,\n",
    "    embedding_size=1408,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhUpWlGAY2uG"
   },
   "source": [
    "### Step 4: Create context_text and context_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "B_EEuuLCe6Y5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"Task: Answer the following questions in detail, providing clear reasoning and evidence from the images and text in bullet points.\n",
    "Instructions:\n",
    "\n",
    "1. **Analyze:** Carefully examine the provided images and text context.\n",
    "2. **Synthesize:** Integrate information from both the visual and textual elements.\n",
    "3. **Reason:**  Deduce logical connections and inferences to address the question.\n",
    "4. **Respond:** Provide a concise, accurate answer in the following format:\n",
    "\n",
    "   * **Question:** [Question]\n",
    "   * **Answer:** [Direct response to the question]\n",
    "   * **Explanation:** [Bullet-point reasoning steps if applicable]\n",
    "   * **Source** [name of the file, page, image from where the information is citied]\n",
    "\n",
    "5. **Ambiguity:** If the context is insufficient to answer, respond \"Not enough context to answer.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# combine all the selected relevant text chunks\n",
    "context_text = [\"Text Context: \"]\n",
    "for key, value in matching_results_chunks_data.items():\n",
    "    context_text.extend(\n",
    "        [\n",
    "            \"Text Source: \",\n",
    "            f\"\"\"file_name: \"{value[\"file_name\"]}\" Page: \"{value[\"page_num\"]}\"\"\",\n",
    "            \"Text\",\n",
    "            value[\"chunk_text\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# combine all the selected relevant images\n",
    "gemini_content = [\n",
    "    instruction,\n",
    "    \"Questions: \",\n",
    "    query,\n",
    "    \"Image Context: \",\n",
    "]\n",
    "for key, value in matching_results_image_fromdescription_data.items():\n",
    "    gemini_content.extend(\n",
    "        [\n",
    "            \"Image Path: \",\n",
    "            value[\"img_path\"],\n",
    "            \"Image Description: \",\n",
    "            value[\"image_description\"],\n",
    "            \"Image:\",\n",
    "            value[\"image_object\"],\n",
    "        ]\n",
    "    )\n",
    "gemini_content.extend(context_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHrtodcBAEu9"
   },
   "source": [
    "### Step 5: Pass context to Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "executionInfo": {
     "elapsed": 23441,
     "status": "ok",
     "timestamp": 1717656213273,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "aZuhtJu7fW4n",
    "outputId": "7dd25f28-f32e-4f30-c402-9f3755f31f2f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span> Why should SaidText be used in the manufacturing world?                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span> SaidText should be used in the manufacturing world because it addresses specific industry challenges by \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>improving efficiency, communication, and data management.                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Manufacturing workers often spend a significant amount of their time writing reports or communicating issues,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>leading to inefficiencies. (\"I lavoratori spendono fino al 50% del loro tempo scrivendo\", Source:            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>saidtext_pitch_ita.pdf, Page 3)                                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>SaidText leverages AI-powered voice recognition to transcribe audio messages into text, reducing manual input\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>and minimizing errors. (Source: saidtext_pitch_ita.pdf, Page 4)                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>This voice-driven approach allows employees to manage tasks, communicate, and resolve problems hands-free,   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>ultimately boosting productivity. (Source: saidtext_pitch_ita.pdf, Page 7)                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>The platform provides real-time updates on factory activities, facilitating timely decision-making for       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>managers. (Source: saidtext_pitch_ita.pdf, Page 4)                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>SaidText integrates with traditional ERP software, enabling workers to input data directly via voice, saving \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>time and reducing bottlenecks. (Source: saidtext_pitch_ita.pdf, Page 7)                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>The platform offers specific benefits for areas like supply chain management, quality control, and issue     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>resolution. (Source: saidtext_pitch_ita.pdf, Page 7)                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Source:</span> saidtext_pitch_ita.pdf, Pages 3, 4, 7                                                                   \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m Why should SaidText be used in the manufacturing world?                                               \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m SaidText should be used in the manufacturing world because it addresses specific industry challenges by \n",
       "\u001b[1;33m   \u001b[0mimproving efficiency, communication, and data management.                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mManufacturing workers often spend a significant amount of their time writing reports or communicating issues,\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mleading to inefficiencies. (\"I lavoratori spendono fino al 50% del loro tempo scrivendo\", Source:            \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0msaidtext_pitch_ita.pdf, Page 3)                                                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mSaidText leverages AI-powered voice recognition to transcribe audio messages into text, reducing manual input\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mand minimizing errors. (Source: saidtext_pitch_ita.pdf, Page 4)                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThis voice-driven approach allows employees to manage tasks, communicate, and resolve problems hands-free,   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0multimately boosting productivity. (Source: saidtext_pitch_ita.pdf, Page 7)                                   \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThe platform provides real-time updates on factory activities, facilitating timely decision-making for       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mmanagers. (Source: saidtext_pitch_ita.pdf, Page 4)                                                           \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mSaidText integrates with traditional ERP software, enabling workers to input data directly via voice, saving \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtime and reducing bottlenecks. (Source: saidtext_pitch_ita.pdf, Page 7)                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThe platform offers specific benefits for areas like supply chain management, quality control, and issue     \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mresolution. (Source: saidtext_pitch_ita.pdf, Page 7)                                                         \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSource:\u001b[0m saidtext_pitch_ita.pdf, Pages 3, 4, 7                                                                   \n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate Gemini response with streaming output\n",
    "rich_Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_15,\n",
    "        model_input=gemini_content,\n",
    "        stream=True,\n",
    "        safety_settings=safety_settings,\n",
    "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0FtXYl1fzKh"
   },
   "source": [
    "### Step 6: Print citations and references [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voThgteH-Tm8"
   },
   "source": [
    "**Optional:** Uncomment to see the detailed citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYRLQ47or1I8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"---------------Matched Images------------------\\n\")\n",
    "# display_images(\n",
    "#     [\n",
    "#         matching_results_image_fromdescription_data[0][\"img_path\"],\n",
    "#         matching_results_image_fromdescription_data[1][\"img_path\"],\n",
    "#     ],\n",
    "#     resize_ratio=0.2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buwd_gp6HJ5K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Image citations. You can check how Gemini generated metadata helped in grounding the answer.\n",
    "\n",
    "# print_text_to_image_citation(\n",
    "#     matching_results_image_fromdescription_data, print_top=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06vYM4MOHJ1-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Text citations\n",
    "\n",
    "# print_text_to_text_citation(\n",
    "#     matching_results_chunks_data,\n",
    "#     print_top=True,\n",
    "#     chunk_text=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIVF-QHuGVDD"
   },
   "source": [
    "### Multimodal RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U82wS4nIB8IS"
   },
   "source": [
    "### More questions with Multimodal QA System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 24067,
     "status": "ok",
     "timestamp": 1717656237338,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "ZAyQVQ54B8X0",
    "outputId": "b79c75b7-0a53-44b0-eb6b-ff565b8b3fca",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span> Where SaidText can be used?                                                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span> SaidText can be used in industrial environments and for customer care applications.                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>SaidText is designed to redefine communication in industrial environments. (saidtext_pitch_ita.pdf, Page 4)  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>SaidText can be used to create white-label customer care applications. (saidtext_pitch_ita.pdf, Page 9)      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Source:</span> saidtext_pitch_ita.pdf, Page 4, 9                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span> Why SaidText is important in production environment?                                                  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span> SaidText is important in production environments because it improves operational efficiency and boosts  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>productivity by enabling hands-free communication, reducing manual data entry, minimizing errors, and providing \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>real-time updates for better decision-making.                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>SaidText uses AI-powered speech recognition to convert voice communications into text, eliminating the need  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>for manual input. (saidtext_pitch_ita.pdf, Page 4, 11)                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>This hands-free approach reduces errors and allows workers to focus on their tasks. (saidtext_pitch_ita.pdf, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>Page 4)                                                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Real-time updates on factory activities facilitate timely decision-making. (saidtext_pitch_ita.pdf, Page 4)  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Source:</span> saidtext_pitch_ita.pdf, Page 4, 11                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Question:</span>  What is the future of SaidText?                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Answer:</span> SaidText is actively developing its product with new features like voice chat, multi-site labels,       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>improved mobile apps, ERP integrations, AI-powered auto-labeling, and plans to expand its market reach.         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Explanation:</span>                                                                                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>The provided roadmap outlines future developments, including product enhancements, marketing initiatives, and\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>hiring plans. (saidtext_pitch_ita.pdf, Page 22)                                                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>These efforts indicate a strong focus on growth and innovation. (saidtext_pitch_ita.pdf, Page 22)            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Source:</span> saidtext_pitch_ita.pdf, Page 22                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m Where SaidText can be used?                                                                           \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m SaidText can be used in industrial environments and for customer care applications.                     \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mSaidText is designed to redefine communication in industrial environments. (saidtext_pitch_ita.pdf, Page 4)  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mSaidText can be used to create white-label customer care applications. (saidtext_pitch_ita.pdf, Page 9)      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSource:\u001b[0m saidtext_pitch_ita.pdf, Page 4, 9                                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m Why SaidText is important in production environment?                                                  \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m SaidText is important in production environments because it improves operational efficiency and boosts  \n",
       "\u001b[1;33m   \u001b[0mproductivity by enabling hands-free communication, reducing manual data entry, minimizing errors, and providing \n",
       "\u001b[1;33m   \u001b[0mreal-time updates for better decision-making.                                                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mSaidText uses AI-powered speech recognition to convert voice communications into text, eliminating the need  \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mfor manual input. (saidtext_pitch_ita.pdf, Page 4, 11)                                                       \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThis hands-free approach reduces errors and allows workers to focus on their tasks. (saidtext_pitch_ita.pdf, \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mPage 4)                                                                                                      \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mReal-time updates on factory activities facilitate timely decision-making. (saidtext_pitch_ita.pdf, Page 4)  \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSource:\u001b[0m saidtext_pitch_ita.pdf, Page 4, 11                                                                      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mQuestion:\u001b[0m  What is the future of SaidText?                                                                      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mAnswer:\u001b[0m SaidText is actively developing its product with new features like voice chat, multi-site labels,       \n",
       "\u001b[1;33m   \u001b[0mimproved mobile apps, ERP integrations, AI-powered auto-labeling, and plans to expand its market reach.         \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mExplanation:\u001b[0m                                                                                                    \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThe provided roadmap outlines future developments, including product enhancements, marketing initiatives, and\n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mhiring plans. (saidtext_pitch_ita.pdf, Page 22)                                                              \n",
       "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mThese efforts indicate a strong focus on growth and innovation. (saidtext_pitch_ita.pdf, Page 22)            \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSource:\u001b[0m saidtext_pitch_ita.pdf, Page 22                                                                         \n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some questions to try\n",
    "# this time we are not passing any images, but just a simple text query.\n",
    "query = \"\"\"Question 1: Where SaidText can be used?\n",
    "\n",
    "Question 2: Why SaidText is important in production environment?\n",
    "\n",
    "Question 3:  What is the future of SaidText?\n",
    " \"\"\"\n",
    "\n",
    "(\n",
    "    response,\n",
    "    matching_results_chunks_data,\n",
    "    matching_results_image_fromdescription_data,\n",
    ") = get_answer_from_qa_system(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    top_n_text=10,\n",
    "    top_n_image=5,\n",
    "    model=multimodal_model_15,\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    ")\n",
    "\n",
    "rich_Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18207,
     "status": "ok",
     "timestamp": 1717656255541,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -330
    },
    "id": "dTHdui6jCHzc",
    "outputId": "316d45cd-b4e3-4e20-db47-b052f32eef24",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "                                                 <span style=\"font-weight: bold; text-decoration: underline\">SaidText Analysis</span>                                                 \n",
       "\n",
       "<span style=\"font-weight: bold\">Question 1: How SaidText can help measuring lead-time in processes?</span>                                                \n",
       "\n",
       "<span style=\"font-weight: bold\">Answer:</span> Not enough context to answer.                                                                              \n",
       "\n",
       "<span style=\"font-weight: bold\">Explanation:</span>                                                                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>While the text mentions SaidText improves operational efficiency, there's no direct mention of measuring        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>lead-time in processes.                                                                                         \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>More information is needed about how SaidText integrates with existing systems or tracks time-stamped data      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>within processes.                                                                                               \n",
       "\n",
       "<span style=\"font-weight: bold\">Source:</span> N/A                                                                                                        \n",
       "\n",
       "<span style=\"font-weight: bold\">Question 2: What should be improved in SaidText? Be creative.</span>                                                      \n",
       "\n",
       "<span style=\"font-weight: bold\">Answer:</span>  Several improvements could enhance SaidText's functionality:                                              \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Real-time language translation:</span>  For global companies, integrating real-time translation within the             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>voice-to-text feature would break down language barriers.                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Voice command integration:</span> Allow users to execute basic commands within the platform using voice, like \"Assign  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>to John\" or \"Mark as urgent.\"                                                                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Sentiment analysis:</span> Incorporate sentiment analysis into the transcribed text to gauge customer emotions and     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>prioritize urgent issues.                                                                                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">AI-powered suggestions for resolutions:</span> Based on the transcribed text and historical data, offer AI-powered     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>suggestions to agents for common problems.                                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Offline mode with synchronization:</span> Allow users to record audio in areas with poor connectivity and sync the data\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>later.                                                                                                          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Integration with project management tools:</span>  Enable seamless integration with popular tools like Asana or Jira   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>for task creation and progress tracking.                                                                        \n",
       "\n",
       "<span style=\"font-weight: bold\">Explanation:</span>  These suggestions enhance communication, efficiency, and analytical capabilities within SaidText.    \n",
       "\n",
       "<span style=\"font-weight: bold\">Source:</span>  N/A                                                                                                       \n",
       "\n",
       "<span style=\"font-weight: bold\">Question 3: What is the story of SaidText?</span>                                                                         \n",
       "\n",
       "<span style=\"font-weight: bold\">Answer:</span>  SaidText is a SaaS platform utilizing AI-driven voice recognition to redefine communication within        \n",
       "industrial settings.                                                                                               \n",
       "\n",
       "<span style=\"font-weight: bold\">Explanation:</span>                                                                                                       \n",
       "\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Problem:</span> The manufacturing industry faces challenges like inefficient text-based communication, leading to      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>errors, bottlenecks, and increased costs.  (\"saidtext_pitch_ita.pdf\", Page 3)                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Solution:</span> SaidText enables hands-free communication by converting audio to text, allowing workers to focus on   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>tasks and input data using voice. This improves efficiency, minimizes errors, and provides real-time insights   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>into factory floor activities. (\"saidtext_pitch_ita.pdf\", Pages 4, 7)                                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Applications:</span> SaidText is applicable for industrial workflows, customer care through white-label solutions, and \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>integration with existing ERP systems. (\"saidtext_pitch_ita.pdf\", Pages 7, 9)                                   \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Technology:</span>  SaidText utilizes AI-powered speech-to-text conversion, categorizes transcribed data, and provides \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>access through an intuitive dashboard. (\"saidtext_pitch_ita.pdf\", Page 11)                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Funding and Recognition:</span>  The company received pre-seed funding from CDP Cassa Depositi e Prestiti and          \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>participated in the Forward Factory acceleration program.  They are also part of the Google for Startups Cloud  \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Program. (\"saidtext_pitch_ita.pdf\", Pages 17, 21)                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Future Roadmap:</span>  SaidText plans to add features like multi-site labels, ERP integration, AI-powered             \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>auto-labeling, and general improvements.  (\"saidtext_pitch_ita.pdf\", Page 22)                                   \n",
       "\n",
       "<span style=\"font-weight: bold\">Source:</span> \"saidtext_pitch_ita.pdf\", various pages as indicated.                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "                                                 \u001b[1;4mSaidText Analysis\u001b[0m                                                 \n",
       "\n",
       "\u001b[1mQuestion 1: How SaidText can help measuring lead-time in processes?\u001b[0m                                                \n",
       "\n",
       "\u001b[1mAnswer:\u001b[0m Not enough context to answer.                                                                              \n",
       "\n",
       "\u001b[1mExplanation:\u001b[0m                                                                                                       \n",
       "\n",
       "\u001b[1;33m • \u001b[0mWhile the text mentions SaidText improves operational efficiency, there's no direct mention of measuring        \n",
       "\u001b[1;33m   \u001b[0mlead-time in processes.                                                                                         \n",
       "\u001b[1;33m • \u001b[0mMore information is needed about how SaidText integrates with existing systems or tracks time-stamped data      \n",
       "\u001b[1;33m   \u001b[0mwithin processes.                                                                                               \n",
       "\n",
       "\u001b[1mSource:\u001b[0m N/A                                                                                                        \n",
       "\n",
       "\u001b[1mQuestion 2: What should be improved in SaidText? Be creative.\u001b[0m                                                      \n",
       "\n",
       "\u001b[1mAnswer:\u001b[0m  Several improvements could enhance SaidText's functionality:                                              \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mReal-time language translation:\u001b[0m  For global companies, integrating real-time translation within the             \n",
       "\u001b[1;33m   \u001b[0mvoice-to-text feature would break down language barriers.                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mVoice command integration:\u001b[0m Allow users to execute basic commands within the platform using voice, like \"Assign  \n",
       "\u001b[1;33m   \u001b[0mto John\" or \"Mark as urgent.\"                                                                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSentiment analysis:\u001b[0m Incorporate sentiment analysis into the transcribed text to gauge customer emotions and     \n",
       "\u001b[1;33m   \u001b[0mprioritize urgent issues.                                                                                       \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mAI-powered suggestions for resolutions:\u001b[0m Based on the transcribed text and historical data, offer AI-powered     \n",
       "\u001b[1;33m   \u001b[0msuggestions to agents for common problems.                                                                      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mOffline mode with synchronization:\u001b[0m Allow users to record audio in areas with poor connectivity and sync the data\n",
       "\u001b[1;33m   \u001b[0mlater.                                                                                                          \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mIntegration with project management tools:\u001b[0m  Enable seamless integration with popular tools like Asana or Jira   \n",
       "\u001b[1;33m   \u001b[0mfor task creation and progress tracking.                                                                        \n",
       "\n",
       "\u001b[1mExplanation:\u001b[0m  These suggestions enhance communication, efficiency, and analytical capabilities within SaidText.    \n",
       "\n",
       "\u001b[1mSource:\u001b[0m  N/A                                                                                                       \n",
       "\n",
       "\u001b[1mQuestion 3: What is the story of SaidText?\u001b[0m                                                                         \n",
       "\n",
       "\u001b[1mAnswer:\u001b[0m  SaidText is a SaaS platform utilizing AI-driven voice recognition to redefine communication within        \n",
       "industrial settings.                                                                                               \n",
       "\n",
       "\u001b[1mExplanation:\u001b[0m                                                                                                       \n",
       "\n",
       "\u001b[1;33m • \u001b[0m\u001b[1mProblem:\u001b[0m The manufacturing industry faces challenges like inefficient text-based communication, leading to      \n",
       "\u001b[1;33m   \u001b[0merrors, bottlenecks, and increased costs.  (\"saidtext_pitch_ita.pdf\", Page 3)                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mSolution:\u001b[0m SaidText enables hands-free communication by converting audio to text, allowing workers to focus on   \n",
       "\u001b[1;33m   \u001b[0mtasks and input data using voice. This improves efficiency, minimizes errors, and provides real-time insights   \n",
       "\u001b[1;33m   \u001b[0minto factory floor activities. (\"saidtext_pitch_ita.pdf\", Pages 4, 7)                                           \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mApplications:\u001b[0m SaidText is applicable for industrial workflows, customer care through white-label solutions, and \n",
       "\u001b[1;33m   \u001b[0mintegration with existing ERP systems. (\"saidtext_pitch_ita.pdf\", Pages 7, 9)                                   \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mTechnology:\u001b[0m  SaidText utilizes AI-powered speech-to-text conversion, categorizes transcribed data, and provides \n",
       "\u001b[1;33m   \u001b[0maccess through an intuitive dashboard. (\"saidtext_pitch_ita.pdf\", Page 11)                                      \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mFunding and Recognition:\u001b[0m  The company received pre-seed funding from CDP Cassa Depositi e Prestiti and          \n",
       "\u001b[1;33m   \u001b[0mparticipated in the Forward Factory acceleration program.  They are also part of the Google for Startups Cloud  \n",
       "\u001b[1;33m   \u001b[0mProgram. (\"saidtext_pitch_ita.pdf\", Pages 17, 21)                                                               \n",
       "\u001b[1;33m • \u001b[0m\u001b[1mFuture Roadmap:\u001b[0m  SaidText plans to add features like multi-site labels, ERP integration, AI-powered             \n",
       "\u001b[1;33m   \u001b[0mauto-labeling, and general improvements.  (\"saidtext_pitch_ita.pdf\", Page 22)                                   \n",
       "\n",
       "\u001b[1mSource:\u001b[0m \"saidtext_pitch_ita.pdf\", various pages as indicated.                                                      \n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some questions to try\n",
    "\n",
    "query = \"\"\"How SaidText can help measuring lead-time in processes?\n",
    "\n",
    "Question 2: What should be improved in SaidText? Be creative.\n",
    "\n",
    "Question 3: What is the story of SaidText?\n",
    "\"\"\"\n",
    "(\n",
    "    response,\n",
    "    matching_results_chunks_data,\n",
    "    matching_results_image_fromdescription_data,\n",
    ") = get_answer_from_qa_system(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    top_n_text=10,\n",
    "    top_n_image=5,\n",
    "    model=multimodal_model_15,\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
    ")\n",
    "\n",
    "rich_Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwNrHCqbi3xi"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05jynhZnkgxn"
   },
   "source": [
    "Congratulations on making it through this multimodal RAG notebook!\n",
    "\n",
    "While multimodal RAG can be quite powerful, note that it can face some limitations:\n",
    "\n",
    "* **Data dependency:** Needs high-quality paired text and visuals.\n",
    "* **Computationally demanding:** Processing multimodal data is resource-intensive.\n",
    "* **Domain specific:** Models trained on general data may not shine in specialized fields like medicine.\n",
    "* **Black box:** Understanding how these models work can be tricky, hindering trust and adoption.\n",
    "\n",
    "\n",
    "Despite these challenges, multimodal RAG represents a significant step towards search and retrieval systems that can handle diverse, multimodal data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "building_DIY_multimodal_qa_system_with_mRAG.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
